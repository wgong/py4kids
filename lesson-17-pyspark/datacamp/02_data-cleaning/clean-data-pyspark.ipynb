{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practice for Data Camp Course [Cleaning Data with Apache Spark in Python](https://www.datacamp.com/courses/cleaning-data-with-apache-spark-in-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "#### Cheatsheets\n",
    "\n",
    "* [PySpark/SQL Cheatsheet by KDnuggets](https://www.kdnuggets.com/2017/11/pyspark-sql-cheat-sheet-big-data-python.html)\n",
    "* [PySpark/RDD Cheatsheet by Datacamp](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_Cheat_Sheet_Python.pdf)\n",
    "* [PySpark Cheatsheet by Gong](https://github.com/wgong/pyspark-cheatsheet)\n",
    "* [PySpark Cheatsheet by Feng](http://web.utk.edu/~wfeng1/doc/cheatSheet_pyspark.pdf)\n",
    "* [PySpark Cheatsheet by Agarwal](https://www.linkedin.com/pulse/pyspark-cheat-sheet-netik-agarwal/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### APIs\n",
    "\n",
    "* [DataFrame/Dataset methods](http://bit.ly/2rKkALY)\n",
    "* [DataFrame functions](http://bit.ly/2DPAycx)\n",
    "* [DataFrameStatFunctions](http://bit.ly/2DPYhJC)\n",
    "* [DataFrameNaFunctions](http://bit.ly/2DPAqd3)\n",
    "* [Column methods](http://bit.ly/2FloFbr)\n",
    "* [SparkSQL builtin functions](https://spark.apache.org/docs/latest/api/sql/index.html#count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Blogs\n",
    "\n",
    "* [Spark Knowledge Base at Databrick](https://kb.databricks.com/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common DataFrame transformations\n",
    "\n",
    "* Select\n",
    "\n",
    "```\n",
    "voter_df.select(voter_df.name)\n",
    "\n",
    "df.select(df.Name, df.Age, F.when(df.Age >= 18, \"Adult\"))\n",
    "\n",
    "df.select(df.Name, df.Age,\n",
    "    .when(df.Age >= 18, \"Adult\")\n",
    "    .when(df.Age < 18, \"Minor\"))\n",
    "    \n",
    "df.select(df.Name, df.Age,\n",
    "    .when(df.Age >= 18, \"Adult\")\n",
    "    .when((df.Age >= 13) & (df.Age < 18), \"Teenager\")\n",
    "    .otherwise(\"Kids\"))    \n",
    "\n",
    "```\n",
    "\n",
    "* Filter/where\n",
    "\n",
    "```\n",
    "voter_df.filter(voter_df.date > '1/1/2019') # or voter_df.where(...)\n",
    "voter_df.filter(voter_df.date.year > 1800)\n",
    "voter_df.filter(voter_df['name'].isNotNull())\n",
    "voter_df.where(voter_df['_c0'].contains('VOTE'))\n",
    "voter_df.where(~ voter_df._c1.isNull())\n",
    "\n",
    "users_df.filter('ID > 3000').select(\"Name\", \"State\")\n",
    "```\n",
    "\n",
    "\n",
    "* withColumn\n",
    "\n",
    "```\n",
    "voter_df.withColumn('year', voter_df.date.year)\n",
    "voter_df.withColumn('upper', F.upper('name'))\n",
    "voter_df.withColumn('splits', F.split('name', ' '))\n",
    "voter df.withColumn('year', voter_df[' c4'].cast(IntegerType()))\n",
    "\n",
    "departures_df = departures_df.withColumn('Duration', departures_df['Duration'].cast(IntegerType()))\n",
    "```\n",
    "\n",
    "* drop\n",
    "\n",
    "`voter_df.drop('unused_column')`\n",
    "\n",
    "* join\n",
    "\n",
    "```\n",
    "parsed_df = spark.read.parquet('parsed_data.parquet')\n",
    "company_df = spark.read.parquet('companies.parquet')\n",
    "verified_df = parsed_df.join(company_df, parsed_df.company == company_df.company)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"clean-data\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff49cac7be0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pyspark.sql.types library\n",
    "# import pyspark.sql.types\n",
    "\n",
    "\n",
    "# Define a new schema using the StructType method\n",
    "people_schema = StructType([\n",
    "  # Define a StructField for each field\n",
    "  StructField('name', StringType(), False),\n",
    "  StructField('age', IntegerType(), False),\n",
    "  StructField('city', StringType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using lazy processing\n",
    "\n",
    "Lazy processing operations will usually return in about the same amount of time regardless of the actual quantity of data. Remember that this is due to Spark not performing any transformations until an action is requested.\n",
    "\n",
    "For this exercise, we'll be defining a Data Frame (aa_dfw_df) and add a couple transformations. Note the amount of time required for the transformations to complete when defined vs when the data is actually queried. These differences may be short, but they will be noticeable. When working with a full Spark cluster with larger quantities of data the difference will be more apparent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gong/projects/py4kids/lesson-17-pyspark/datacamp/02_data-cleaning\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AA_DFW_2014_Departures_Short.csv.gz  airports.csv.gz\r\n",
      "AA_DFW_2015_Departures_Short.csv.gz  DallasCouncilVoters.csv.gz\r\n",
      "AA_DFW_2016_Departures_Short.csv.gz  DallasCouncilVotes.csv.gz\r\n",
      "AA_DFW_2017_Departures_Short.csv.gz  flights_small.csv.gz\r\n",
      "AA_DFW_sample1.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./data/AA_DFW_2017_Departures_Short.csv.gz\"\n",
    "# Load the CSV file\n",
    "aa_dfw_df = spark.read.format('csv').options(Header=True).load(file_path)\n",
    "\n",
    "# or simply\n",
    "# aa_dfw_df = spark.read.csv(file_path,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|       01/01/2017|         0005|                HNL|                          537|\n",
      "|       01/01/2017|         0007|                OGG|                          498|\n",
      "|       01/01/2017|         0037|                SFO|                          241|\n",
      "|       01/01/2017|         0043|                DTW|                          134|\n",
      "|       01/01/2017|         0051|                STL|                           88|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aa_dfw_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schema and Describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date (MM/DD/YYYY): string (nullable = true)\n",
      " |-- Flight Number: string (nullable = true)\n",
      " |-- Destination Airport: string (nullable = true)\n",
      " |-- Actual elapsed time (Minutes): string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aa_dfw_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date (MM/DD/YYYY)',\n",
       " 'Flight Number',\n",
       " 'Destination Airport',\n",
       " 'Actual elapsed time (Minutes)']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa_dfw_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date (MM/DD/YYYY)',\n",
       " 'Flight Number',\n",
       " 'Destination Airport',\n",
       " 'Actual elapsed time (Minutes)']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa_dfw_df.schema.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike Pandas, pySpark Dataframe has no attribute .shape, but easy to create a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape(df):\n",
    "    return df.count(), len(df.columns), df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(139358,\n",
       " 4,\n",
       " ['Date (MM/DD/YYYY)',\n",
       "  'Flight Number',\n",
       "  'Destination Airport',\n",
       "  'Actual elapsed time (Minutes)'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape(aa_dfw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|Destination Airport|\n",
      "+-------+-------------------+\n",
      "|  count|             139358|\n",
      "|   mean|               null|\n",
      "| stddev|               null|\n",
      "|    min|                ABQ|\n",
      "|    max|                XNA|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# describe one column\n",
    "aa_dfw_df.describe('Destination Airport').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+\n",
      "|summary|Date (MM/DD/YYYY)|     Flight Number|\n",
      "+-------+-----------------+------------------+\n",
      "|  count|           139358|            139358|\n",
      "|   mean|             null|1580.4799867965958|\n",
      "| stddev|             null|  817.419749529198|\n",
      "|    min|       01/01/2017|              0005|\n",
      "|    max|       12/31/2017|              2779|\n",
      "+-------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# describe multi-columns\n",
    "aa_dfw_df.describe(['Date (MM/DD/YYYY)','Flight Number']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+-------------------+-----------------------------+\n",
      "|summary|Date (MM/DD/YYYY)|     Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
      "+-------+-----------------+------------------+-------------------+-----------------------------+\n",
      "|  count|           139358|            139358|             139358|                       139358|\n",
      "|   mean|             null|1580.4799867965958|               null|           151.99931112673832|\n",
      "| stddev|             null|  817.419749529198|               null|            64.25904268384629|\n",
      "|    min|       01/01/2017|              0005|                ABQ|                            0|\n",
      "|    max|       12/31/2017|              2779|                XNA|                           99|\n",
      "+-------+-----------------+------------------+-------------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# describe all columns\n",
    "aa_dfw_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### renaming columns\n",
    "\n",
    "https://stackoverflow.com/questions/34077353/how-to-change-dataframe-column-names-in-pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-----------------------------+-------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Actual elapsed time (Minutes)|airport|\n",
      "+-----------------+-------------+-----------------------------+-------+\n",
      "|       01/01/2017|         0005|                          537|    hnl|\n",
      "|       01/01/2017|         0007|                          498|    ogg|\n",
      "|       01/01/2017|         0037|                          241|    sfo|\n",
      "|       01/01/2017|         0043|                          134|    dtw|\n",
      "|       01/01/2017|         0051|                           88|    stl|\n",
      "+-----------------+-------------+-----------------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add the airport column using the F.lower() method\n",
    "aa_dfw_df = aa_dfw_df.withColumn('airport', F.lower(aa_dfw_df['Destination Airport']))\n",
    "\n",
    "# Drop the Destination Airport column\n",
    "aa_dfw_df = aa_dfw_df.drop(aa_dfw_df['Destination Airport'])\n",
    "\n",
    "# Show the DataFrame\n",
    "aa_dfw_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------+-------+\n",
      "|      Date|Flight Number|FlightTime|airport|\n",
      "+----------+-------------+----------+-------+\n",
      "|01/01/2017|         0005|       537|    hnl|\n",
      "|01/01/2017|         0007|       498|    ogg|\n",
      "|01/01/2017|         0037|       241|    sfo|\n",
      "|01/01/2017|         0043|       134|    dtw|\n",
      "|01/01/2017|         0051|        88|    stl|\n",
      "+----------+-------------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aa_dfw_df = aa_dfw_df\\\n",
    "            .withColumnRenamed(\"Date (MM/DD/YYYY)\", \"Date\")\\\n",
    "            .withColumnRenamed(\"Actual elapsed time (Minutes)\", \"FlightTime\")\n",
    "\n",
    "aa_dfw_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+-------+\n",
      "|      date|flight_no|flight_time|airport|\n",
      "+----------+---------+-----------+-------+\n",
      "|01/01/2017|     0005|        537|    hnl|\n",
      "|01/01/2017|     0007|        498|    ogg|\n",
      "|01/01/2017|     0037|        241|    sfo|\n",
      "|01/01/2017|     0043|        134|    dtw|\n",
      "|01/01/2017|     0051|         88|    stl|\n",
      "+----------+---------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# alias column\n",
    "aa_dfw_df = aa_dfw_df.select(F.col(\"Date\").alias(\"date\"), \n",
    "                 F.col(\"Flight Number\").alias(\"flight_no\"),\n",
    "                 F.col(\"FlightTime\").alias(\"flight_time\"),\n",
    "                 F.col(\"airport\")\n",
    "                )\n",
    "\n",
    "aa_dfw_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### selectExpr is flexible,powerful "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+-------+\n",
      "|      Date|FlightNum|FlightTime|Airport|\n",
      "+----------+---------+----------+-------+\n",
      "|01/01/2017|     0005|       537|    hnl|\n",
      "|01/01/2017|     0007|       498|    ogg|\n",
      "|01/01/2017|     0037|       241|    sfo|\n",
      "|01/01/2017|     0043|       134|    dtw|\n",
      "|01/01/2017|     0051|        88|    stl|\n",
      "+----------+---------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aa_dfw_df1 = aa_dfw_df.selectExpr(\"date as Date\", \"flight_no as FlightNum\", \"flight_time as FlightTime\", \"airport as Airport\")\n",
    "\n",
    "aa_dfw_df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|avg_flight_time|airports|\n",
      "+---------------+--------+\n",
      "|        151.999|      90|\n",
      "+---------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aa_dfw_df1.selectExpr(\"round(avg(FlightTime),3) as avg_flight_time\", \"count(distinct(Airport)) as airports\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_dfw_df.sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = aa_dfw_df.sample(False, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+-------+\n",
      "|      date|flight_no|flight_time|airport|\n",
      "+----------+---------+-----------+-------+\n",
      "|01/01/2017|     0475|        185|    san|\n",
      "|01/01/2017|     0618|        176|    las|\n",
      "|01/01/2017|     1292|         55|    tul|\n",
      "|01/01/2017|     2179|        139|    guc|\n",
      "|01/01/2017|     2386|         51|    okc|\n",
      "+----------+---------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1492, 4, ['date', 'flight_no', 'flight_time', 'airport'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = aa_dfw_df.sample(False, 0.01, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 Count: 1492\n",
      "df2 Count: 1423\n"
     ]
    }
   ],
   "source": [
    "# View the row count of df1 and df2\n",
    "print(\"df1 Count: %d\" % df1.count())\n",
    "print(\"df2 Count: %d\" % df2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine 2 DataFrames into one\n",
    "df3 = df1.union(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2915, 4, ['date', 'flight_no', 'flight_time', 'airport'])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving a DataFrame in Parquet format\n",
    "\n",
    "When working with Spark, you'll often start with CSV, JSON, or other data sources. This provides a lot of flexibility for the types of data to load, but it is not an optimal format for Spark. The Parquet format is a columnar data store, allowing Spark to use predicate pushdown. This means Spark will only process the data necessary to complete the operations you define versus reading the entire dataset. This gives Spark more flexibility in accessing the data and often drastically improves performance on large datasets.\n",
    "\n",
    "In this exercise, we're going to practice creating a new Parquet file and then process some data from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileout_path = './data/AA_DFW_sample1.parquet'\n",
    "# Save the df3 DataFrame in Parquet format\n",
    "df3.write.parquet(fileout_path, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2915\n"
     ]
    }
   ],
   "source": [
    "df3_1 = spark.read.parquet(fileout_path)\n",
    "# Read the Parquet file into a new DataFrame and run a count\n",
    "print(df3_1.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+-------+\n",
      "|      date|flight_no|flight_time|airport|\n",
      "+----------+---------+-----------+-------+\n",
      "|01/01/2017|     0475|        185|    san|\n",
      "|01/01/2017|     0618|        176|    las|\n",
      "|01/01/2017|     1292|         55|    tul|\n",
      "|01/01/2017|     2179|        139|    guc|\n",
      "|01/01/2017|     2386|         51|    okc|\n",
      "+----------+---------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3_1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL and Parquet\n",
    "\n",
    "Parquet files are perfect as a backing data store for SQL queries in Spark. While it is possible to run the same queries directly via Spark's Python functions, sometimes it's easier to run SQL queries alongside the Python options.\n",
    "\n",
    "For this example, we're going to read in the Parquet file we created in the last exercise and register it as a SQL table. Once registered, we'll run a quick query against the table (aka, the Parquet file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create table from dataframe\n",
    "df3_1.createOrReplaceTempView(\"flights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use SQL to rename columns\n",
    "flights = spark.sql(\"\"\"SELECT Date, \n",
    "                        flight_no AS FlightNo, \n",
    "                        flight_time AS FlightTime, \n",
    "                        airport AS Airport \n",
    "                        from flights\n",
    "                        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+-------+\n",
      "|      Date|FlightNo|FlightTime|Airport|\n",
      "+----------+--------+----------+-------+\n",
      "|01/01/2017|    0475|       185|    san|\n",
      "|01/01/2017|    0618|       176|    las|\n",
      "|01/01/2017|    1292|        55|    tul|\n",
      "|01/01/2017|    2179|       139|    guc|\n",
      "|01/01/2017|    2386|        51|    okc|\n",
      "+----------+--------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average flight time is: 153\n"
     ]
    }
   ],
   "source": [
    "# Run a SQL query of the average flight duration\n",
    "avg_duration = spark.sql('SELECT avg(flight_time) from flights').collect()[0]\n",
    "print('The average flight time is: %d' % avg_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register table using SQLContext\n",
    "sqlContext = SQLContext(spark.sparkContext, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.registerDataFrameAsTable(flights, \"flights_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_df = sqlContext.sql(\"SELECT * from flights_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+-------+\n",
      "|      Date|FlightNo|FlightTime|Airport|\n",
      "+----------+--------+----------+-------+\n",
      "|01/01/2017|    0475|       185|    san|\n",
      "|01/01/2017|    0618|       176|    las|\n",
      "|01/01/2017|    1292|        55|    tul|\n",
      "|01/01/2017|    2179|       139|    guc|\n",
      "|01/01/2017|    2386|        51|    okc|\n",
      "+----------+--------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering column content with Python\n",
    "\n",
    "You've looked at using various operations on DataFrame columns - now you can modify a real dataset. The DataFrame voter_df contains information regarding the voters on the Dallas City Council from the past few years. This truncated DataFrame contains the date of the vote being cast and the name and position of the voter. Your manager has asked you to clean this data so it can later be integrated into some desired reports. The primary task is to remove any null entries or odd characters and return a specific set of voters where you can validate their information.\n",
    "\n",
    "This is often one of the first steps in data cleaning - removing anything that is obviously outside the format. For this dataset, make sure to look at the original data and see what looks out of place for the VOTER_NAME column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./data/DallasCouncilVoters.csv.gz\"\n",
    "voter_df = spark.read.format('csv').options(Header=True).load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|\n",
      "+----------+-------------+-------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|\n",
      "+----------+-------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voter_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|VOTER_NAME                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Tennell Atkins                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|  the  final   2018 Assessment Plan and the 2018 Assessment  Roll  (to  be  kept  on  file  with  the  City  Secretary);  establishing  classifications  for   the   apportionment   of   costs and the methods of assessing special assessments for the services and improvements to property in the District;  closing  the  hearing  and  levying  a  special  assessment  on  property  in  the  District|\n",
      "|Scott Griggs                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|Scott  Griggs                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|Sandy Greyson                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the distinct VOTER_NAME entries\n",
    "# voter_df.select(voter_df['VOTER_NAME']).distinct().show(5, truncate=False)\n",
    "voter_df.select('VOTER_NAME').distinct().show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter voter_df where the VOTER_NAME is 1-20 characters in length\n",
    "voter_df = voter_df.filter('length(VOTER_NAME) > 0 and length(VOTER_NAME) < 20')\n",
    "# voter_df = voter_df.where('length(VOTER_NAME) > 0 and length(VOTER_NAME) < 20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|\n",
      "+----------+-------------+-------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|\n",
      "+----------+-------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voter_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out voter_df where the VOTER_NAME contains an underscore\n",
    "voter_df = voter_df.filter(~ F.col('VOTER_NAME').contains(\"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|\n",
      "+----------+-------------+-------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|\n",
      "+----------+-------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voter_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying DataFrame columns\n",
    "\n",
    "Previously, you filtered out any rows that didn't conform to something generally resembling a name. Now based on your earlier work, your manager has asked you to create two new columns - first_name and last_name. She asks you to split the VOTER_NAME column into words on any space character. You'll treat the last word as the last_name, and all other words as the first_name. You'll be using some new functions in this exercise including .split(), .size(), and .getItem().\n",
    "\n",
    "Please note that these operations are always somewhat specific to the use case. Having your data conform to a format often matters more than the specific details of the format. Rarely is a data cleaning task meant just for one person - matching a defined format allows for easier sharing of the data later (ie, Paul doesn't need to worry about names - Mary already cleaned the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+----------+---------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|first_name|last_name|\n",
      "+----------+-------------+-------------------+----------+---------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|    Philip| Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|   Michael| Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|      Adam|  Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     Casey|   Thomas|\n",
      "+----------+-------------+-------------------+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a new column called splits separated on whitespace\n",
    "voter_df = voter_df.withColumn(\"splits\", F.split(voter_df.VOTER_NAME, '\\s+'))\n",
    "\n",
    "# Create a new column called first_name based on the first item in splits\n",
    "voter_df = voter_df.withColumn(\"first_name\", voter_df.splits.getItem(0))\n",
    "\n",
    "# Get the last entry of the splits list and create a column called last_name\n",
    "voter_df = voter_df.withColumn(\"last_name\", voter_df.splits.getItem(F.size('splits') - 1))\n",
    "\n",
    "# Drop the splits column\n",
    "voter_df = voter_df.drop('splits')\n",
    "\n",
    "# Show the voter_df DataFrame\n",
    "voter_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### when() example\n",
    "\n",
    "The when() clause lets you conditionally modify a Data Frame based on its content. You'll want to modify our voter_df DataFrame to add a random number to any voting member that is defined as a \"Councilmember\".\n",
    "\n",
    "The voter_df DataFrame is defined and available to you. The pyspark.sql.functions library is available as F. You can use F.rand() to generate the random value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+----------+---------+----------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|first_name|last_name|random_val|\n",
      "+----------+-------------+-------------------+----------+---------+----------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|     0.954|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|    Philip| Kingston|     0.957|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|   Michael| Rawlings|      null|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|      Adam|  Medrano|     0.528|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     Casey|   Thomas|     0.082|\n",
      "+----------+-------------+-------------------+----------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a column to voter_df for any voter with the title **Councilmember**\n",
    "voter_df = voter_df.withColumn('random_val',\n",
    "                               F.when(voter_df['TITLE'].contains(\"Councilmember\"), F.round(F.rand(),3)))\n",
    "\n",
    "# Show some of the DataFrame rows, noting whether the when clause worked\n",
    "voter_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When / Otherwise\n",
    "\n",
    "This requirement is similar to the last, but now you want to add multiple values based on the voter's position. Modify your voter_df DataFrame to add a random number to any voting member that is defined as a Councilmember. Use 2 for the Mayor and 0 for anything other position.\n",
    "\n",
    "The voter_df Data Frame is defined and available to you. The pyspark.sql.functions library is available as F. You can use F.rand() to generate the random value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+----------+---------+----------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|first_name|last_name|random_val|\n",
      "+----------+-------------+-------------------+----------+---------+----------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|     0.101|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|    Philip| Kingston|     0.292|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|   Michael| Rawlings|       2.0|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|      Adam|  Medrano|     0.893|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     Casey|   Thomas|     0.852|\n",
      "+----------+-------------+-------------------+----------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a column to voter_df for a voter based on their position\n",
    "voter_df = voter_df.withColumn('random_val',\n",
    "            F.when(voter_df.TITLE == 'Councilmember', F.round(F.rand(),3) )\n",
    "            .when(voter_df.TITLE == 'Mayor', 2)\n",
    "            .otherwise(0))\n",
    "\n",
    "# Show some of the DataFrame rows\n",
    "voter_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------------+----------+---------+----------+\n",
      "|      DATE|               TITLE|       VOTER_NAME|first_name|last_name|random_val|\n",
      "+----------+--------------------+-----------------+----------+---------+----------+\n",
      "|04/25/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|04/25/2018|       Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|06/20/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|06/20/2018|       Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|06/20/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "+----------+--------------------+-----------------+----------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the .filter() clause with random_val\n",
    "voter_df.filter(voter_df.random_val == 0).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### working with Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateDF = spark.range(3)\\\n",
    "    .withColumn(\"today\", F.current_date())\\\n",
    "    .withColumn(\"now\", F.current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+\n",
      "| id|     today|                 now|\n",
      "+---+----------+--------------------+\n",
      "|  0|2019-09-14|2019-09-14 15:19:...|\n",
      "|  1|2019-09-14|2019-09-14 15:19:...|\n",
      "|  2|2019-09-14|2019-09-14 15:19:...|\n",
      "+---+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- today: date (nullable = false)\n",
      " |-- now: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF.createOrReplaceTempView(\"dateTable\")\n",
    "dateDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+\n",
      "|     today|5days_later| 5days_ago|\n",
      "+----------+-----------+----------+\n",
      "|2019-09-14| 2019-09-19|2019-09-09|\n",
      "+----------+-----------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF.select(\"today\", F.date_add(F.col(\"today\"),5).alias(\"5days_later\"), F.date_sub(F.col(\"today\"),5).alias(\"5days_ago\")).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|datediff(week_ago, today)|\n",
      "+-------------------------+\n",
      "|                       -7|\n",
      "|                       -7|\n",
      "+-------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF = dateDF.withColumn(\"week_ago\", F.date_sub(F.col(\"today\"),7))\n",
    "\n",
    "dateDF.select(F.datediff(F.col(\"week_ago\"), F.col(\"today\"))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+----------+\n",
      "| id|     today|                 now|  week_ago|\n",
      "+---+----------+--------------------+----------+\n",
      "|  0|2019-09-14|2019-09-14 15:27:...|2019-09-07|\n",
      "|  1|2019-09-14|2019-09-14 15:27:...|2019-09-07|\n",
      "+---+----------+--------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### to_date(), to_timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|month_diff|\n",
      "+----------+\n",
      "|      12.0|\n",
      "+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF.select(\n",
    "    F.to_date(F.lit(\"2016-01-01\")).alias(\"start\"),\n",
    "    F.to_date(F.lit(\"2017-01-01\")).alias(\"end\") )\\\n",
    "    .select(F.months_between(F.col(\"end\"), F.col(\"start\")).alias(\"month_diff\")).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----+\n",
      "|       dt1|       dt2| dt3|\n",
      "+----------+----------+----+\n",
      "|2019-01-02|2019-02-13|null|\n",
      "|2019-01-02|2019-02-13|null|\n",
      "+----------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateFormat = \"yyyy-MM-dd\"\n",
    "cleanDateDF = spark.range(2).select(\n",
    "    F.to_date(F.lit(\"2019-01-02\"), dateFormat).alias(\"dt1\"),\n",
    "    F.to_date(F.lit(\"2019-02-13\"), dateFormat).alias(\"dt2\"),\n",
    "    F.to_date(F.lit(\"2019-13-02\"), dateFormat).alias(\"dt3\")\n",
    "    )\n",
    "cleanDateDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|to_timestamp(`dt1`, 'yyyy-MM-dd')|\n",
      "+---------------------------------+\n",
      "|              2019-01-02 00:00:00|\n",
      "|              2019-01-02 00:00:00|\n",
      "+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanDateDF.select(F.to_timestamp(F.col(\"dt1\"), dateFormat)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----+\n",
      "|       dt1|       dt2| dt3|\n",
      "+----------+----------+----+\n",
      "|2019-01-02|2019-02-13|null|\n",
      "|2019-01-02|2019-02-13|null|\n",
      "+----------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanDateDF.filter(F.col(\"dt1\") > F.lit(\"2018-12-30\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+\n",
      "|       dt1|       dt2|       dt3|\n",
      "+----------+----------+----------+\n",
      "|2018-01-02|2019-12-13|2008-03-03|\n",
      "|2018-01-02|2019-12-13|2008-03-03|\n",
      "+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanDateDF2 = spark.range(2).select(\n",
    "    F.to_date(F.lit(\"2018-01-02\"), dateFormat).alias(\"dt1\"),\n",
    "    F.to_date(F.lit(\"2019-12-13\"), dateFormat).alias(\"dt2\"),\n",
    "    F.to_date(F.lit(\"2008-03-03\"), dateFormat).alias(\"dt3\")\n",
    "    )\n",
    "cleanDateDF2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+\n",
      "|       dt1|       dt2|       dt3|\n",
      "+----------+----------+----------+\n",
      "|2019-01-02|2019-02-13|      null|\n",
      "|2019-01-02|2019-02-13|      null|\n",
      "|2018-01-02|2019-12-13|2008-03-03|\n",
      "|2018-01-02|2019-12-13|2008-03-03|\n",
      "+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanDateDF = cleanDateDF.union(cleanDateDF2)\n",
    "cleanDateDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+\n",
      "|       dt1|       dt2|       dt3|\n",
      "+----------+----------+----------+\n",
      "|2018-01-02|2019-12-13|2008-03-03|\n",
      "|2018-01-02|2019-12-13|2008-03-03|\n",
      "+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanDateDF.na.drop(\"any\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+\n",
      "|       dt1|       dt2|       dt3|\n",
      "+----------+----------+----------+\n",
      "|2019-01-02|2019-02-13|      null|\n",
      "|2019-01-02|2019-02-13|      null|\n",
      "|2018-01-02|2019-12-13|2008-03-03|\n",
      "|2018-01-02|2019-12-13|2008-03-03|\n",
      "+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanDateDF.na.fill('2019-09-14', subset=[\"dt3\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User defined functions or UDFs\n",
    "* Python method\n",
    "* Wrapped via the pyspark.sql.functions.udf method\n",
    "* Stored as a variable\n",
    "* Called like a normal Spark function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverseString(mystr):\n",
    "    return mystr[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'allebanna'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverseString(\"annabella\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "argument-less udf\n",
    "```\n",
    "def sortingCap():\n",
    "    return random.choice(['G', 'H', 'R', 'S'])\n",
    "udfSortingCap = udf(sortingCap, StringType())\n",
    "user_df = user_df.withColumn('Class', udfSortingCap())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFirstAndMiddle(names):\n",
    "    # Return a space separated string of names\n",
    "    return ' '.join(names[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Mid'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getFirstAndMiddle(['First', \"Mid\", \"Last\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./data/DallasCouncilVoters.csv.gz\"\n",
    "voter_df = spark.read.format('csv').options(Header=True).load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column called splits separated on whitespace\n",
    "voter_df = voter_df.withColumn(\"splits\", F.split(voter_df.VOTER_NAME, '\\s+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_df = voter_df.withColumn(\"first_name\", voter_df.splits.getItem(0))\n",
    "# Get the last entry of the splits list and create a column called last_name\n",
    "voter_df = voter_df.withColumn(\"last_name\", voter_df.splits.getItem(F.size('splits') - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+--------------------+----------+---------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|              splits|first_name|last_name|\n",
      "+----------+-------------+-------------------+--------------------+----------+---------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|[Philip, T., King...|    Philip| Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|[Michael, S., Raw...|   Michael| Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     [Casey, Thomas]|     Casey|   Thomas|\n",
      "+----------+-------------+-------------------+--------------------+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voter_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+---------+---------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|last_name|first_and_middle_name|\n",
      "+----------+-------------+-------------------+---------+---------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|    Gates|          Jennifer S.|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston| Kingston|            Philip T.|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings| Rawlings|           Michael S.|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|  Medrano|                 Adam|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|   Thomas|                Casey|\n",
      "+----------+-------------+-------------------+---------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def getFirstAndMiddle(names):\n",
    "    ret_val = ''\n",
    "    if names:\n",
    "        # Return a space separated string of names\n",
    "        ret_val = ' '.join(names[:-1])\n",
    "    return ret_val\n",
    "\n",
    "# Define the method as a UDF\n",
    "udfFirstAndMiddle = F.udf(getFirstAndMiddle, F.StringType())\n",
    "\n",
    "# Create a new column using your UDF\n",
    "voter_df = voter_df.withColumn('first_and_middle_name', udfFirstAndMiddle(F.col('splits')))\n",
    "\n",
    "# Drop the unecessary columns then show the DataFrame\n",
    "voter_df = voter_df.drop('first_name')\n",
    "voter_df = voter_df.drop('splits')\n",
    "\n",
    "voter_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding an ID Field\n",
    "\n",
    "When working with data, you sometimes only want to access certain fields and perform various operations. In this case, find all the unique voter names from the DataFrame and add a unique ID number. Remember that Spark IDs are assigned based on the DataFrame partition - as such the ID values may be much greater than the actual number of rows in the DataFrame.\n",
    "\n",
    "With Spark's lazy processing, the IDs are not actually generated until an action is performed and can be somewhat random depending on the size of the dataset.\n",
    "\n",
    "The spark session and a Spark DataFrame df containing the DallasCouncilVotes.csv.gz file are available in your workspace. The pyspark.sql.functions library is available under the alias F."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./data/DallasCouncilVotes.csv.gz\"\n",
    "df = spark.read.format('csv').options(Header=True).load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+---------+--------+-------------+-------------------+---------+------------------+-----------------------+------------------+--------------------+\n",
      "|      DATE|AGENDA_ITEM_NUMBER|ITEM_TYPE|DISTRICT|        TITLE|         VOTER NAME|VOTE CAST|FINAL ACTION TAKEN|AGENDA ITEM DESCRIPTION|         AGENDA_ID|             VOTE_ID|\n",
      "+----------+------------------+---------+--------+-------------+-------------------+---------+------------------+-----------------------+------------------+--------------------+\n",
      "|02/08/2017|                 1|   AGENDA|      13|Councilmember|  Jennifer S. Gates|      N/A|  NO ACTION NEEDED|          Call to Order|020817__Special__1|020817__Special__...|\n",
      "|02/08/2017|                 1|   AGENDA|      14|Councilmember| Philip T. Kingston|      N/A|  NO ACTION NEEDED|          Call to Order|020817__Special__1|020817__Special__...|\n",
      "|02/08/2017|                 1|   AGENDA|      15|        Mayor|Michael S. Rawlings|      N/A|  NO ACTION NEEDED|          Call to Order|020817__Special__1|020817__Special__...|\n",
      "|02/08/2017|                 1|   AGENDA|       2|Councilmember|       Adam Medrano|      N/A|  NO ACTION NEEDED|          Call to Order|020817__Special__1|020817__Special__1_2|\n",
      "|02/08/2017|                 1|   AGENDA|       3|Councilmember|   Casey Thomas, II|      N/A|  NO ACTION NEEDED|          Call to Order|020817__Special__1|020817__Special__1_3|\n",
      "+----------+------------------+---------+--------+-------------+-------------------+---------+------------------+-----------------------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all the unique council voters\n",
    "voter_df = df.select(df[\"VOTER NAME\"]).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 1)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape(voter_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 36 rows in the voter_df DataFrame.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count the rows in voter_df\n",
    "print(\"\\nThere are %d rows in the voter_df DataFrame.\\n\" % voter_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a ROW_ID\n",
    "voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|          VOTER NAME|     ROW_ID|\n",
      "+--------------------+-----------+\n",
      "|      Tennell Atkins| 8589934592|\n",
      "|  the  final   20...|25769803776|\n",
      "|        Scott Griggs|34359738368|\n",
      "|       Scott  Griggs|42949672960|\n",
      "|       Sandy Greyson|51539607552|\n",
      "+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voter_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|          VOTER NAME|       ROW_ID|\n",
      "+--------------------+-------------+\n",
      "|        Lee Kleinman|1709396983808|\n",
      "|  the  final  201...|1700807049217|\n",
      "|         Erik Wilson|1700807049216|\n",
      "|  the  final   20...|1683627180032|\n",
      "| Carolyn King Arnold|1632087572480|\n",
      "| Rickey D.  Callahan|1597727834112|\n",
      "|   the   final  2...|1443109011456|\n",
      "|    Monica R. Alonzo|1382979469312|\n",
      "|     Lee M. Kleinman|1228360646656|\n",
      "|   Jennifer S. Gates|1194000908288|\n",
      "+--------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the rows with 10 highest IDs in the set\n",
    "voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voter_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_df_single = voter_df.repartition(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voter_df_single.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 200 partitions in the voter_df DataFrame.\n",
      "\n",
      "\n",
      "There are 1 partitions in the voter_df_single DataFrame.\n",
      "\n",
      "+--------------------+-------------+\n",
      "|          VOTER NAME|       ROW_ID|\n",
      "+--------------------+-------------+\n",
      "|        Lee Kleinman|1709396983808|\n",
      "|  the  final  201...|1700807049217|\n",
      "|         Erik Wilson|1700807049216|\n",
      "|  the  final   20...|1683627180032|\n",
      "| Carolyn King Arnold|1632087572480|\n",
      "| Rickey D.  Callahan|1597727834112|\n",
      "|   the   final  2...|1443109011456|\n",
      "|    Monica R. Alonzo|1382979469312|\n",
      "|     Lee M. Kleinman|1228360646656|\n",
      "|   Jennifer S. Gates|1194000908288|\n",
      "+--------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+------+\n",
      "|          VOTER NAME|ROW_ID|\n",
      "+--------------------+------+\n",
      "|        Lee Kleinman|    35|\n",
      "|  the  final  201...|    34|\n",
      "|         Erik Wilson|    33|\n",
      "|  the  final   20...|    32|\n",
      "| Carolyn King Arnold|    31|\n",
      "| Rickey D.  Callahan|    30|\n",
      "|   the   final  2...|    29|\n",
      "|    Monica R. Alonzo|    28|\n",
      "|     Lee M. Kleinman|    27|\n",
      "|   Jennifer S. Gates|    26|\n",
      "+--------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the number of partitions in each DataFrame\n",
    "print(\"\\nThere are %d partitions in the voter_df DataFrame.\\n\" % voter_df.rdd.getNumPartitions())\n",
    "print(\"\\nThere are %d partitions in the voter_df_single DataFrame.\\n\" % voter_df_single.rdd.getNumPartitions())\n",
    "\n",
    "# Add a ROW_ID field to each DataFrame\n",
    "voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "voter_df_single = voter_df_single.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "\n",
    "# Show the top 10 IDs in each DataFrame \n",
    "voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)\n",
    "voter_df_single.orderBy(voter_df_single.ROW_ID.desc()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More ID tricks\n",
    "\n",
    "Once you define a Spark process, you'll likely want to use it many times. Depending on your needs, you may want to start your IDs at a certain value so there isn't overlap with previous runs of the Spark task. This behavior is similar to how IDs would behave in a relational database. You have been given the task to make sure that the IDs output from a monthly Spark task start at the highest value from the previous month.\n",
    "\n",
    "The spark session and two DataFrames, voter_df_march and voter_df_april, are available in your workspace. The pyspark.sql.functions library is available under the alias F."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the highest ROW_ID and save it in previous_max_ID\n",
    "previous_max_ID = voter_df_march.select('ROW_ID').rdd.max()[0]\n",
    "\n",
    "# Add a ROW_ID column to voter_df_april starting at the desired value\n",
    "voter_df_april = voter_df_april.withColumn('ROW_ID', previous_max_ID + F.monotonically_increasing_id())\n",
    "\n",
    "# Show the ROW_ID from both DataFrames and compare\n",
    "voter_df_march.select('ROW_ID').show()\n",
    "voter_df_april.select('ROW_ID').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching a DataFrame\n",
    "\n",
    "You've been assigned a task that requires running several analysis operations on a DataFrame. You've learned that caching can improve performance when reusing DataFrames and would like to implement it.\n",
    "\n",
    "You'll be working with a new dataset consisting of airline departure information. It may have repetitive data and will need to be de-duplicated.\n",
    "\n",
    "The DataFrame departures_df is defined, but no actions have been performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voter_df.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voter_df = voter_df.cache()\n",
    "voter_df.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voter_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voter_df = voter_df.unpersist()\n",
    "voter_df.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voter_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./data/AA_DFW_2014_Departures_Short.csv.gz\"\n",
    "departures_df = spark.read.format('csv').options(Header=True).load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting 157198 rows took 2.452549 seconds\n",
      "Counting 157198 rows again took 0.439769 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Add caching to the unique rows in departures_df\n",
    "departures_df = departures_df.distinct().cache()\n",
    "\n",
    "# Count the unique rows in departures_df, noting how long the operation takes\n",
    "print(\"Counting %d rows took %f seconds\" % (departures_df.count(), time.time() - start_time))\n",
    "\n",
    "# Count the rows again, noting the variance in time of a cached DataFrame\n",
    "start_time = time.time()\n",
    "print(\"Counting %d rows again took %f seconds\" % (departures_df.count(), time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Counting 157198 rows took 2.452549 seconds\n",
    "Counting 157198 rows again took 0.439769 seconds\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing a DataFrame from cache\n",
    "\n",
    "You've finished the analysis tasks with the departures_df DataFrame, but have some other processing to do. You'd like to remove the DataFrame from the cache to prevent any excess memory usage on your cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is departures_df cached?: True\n",
      "Removing departures_df from cache\n",
      "Is departures_df cached?: False\n"
     ]
    }
   ],
   "source": [
    "# Determine if departures_df is in the cache\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)\n",
    "print(\"Removing departures_df from cache\")\n",
    "\n",
    "# Remove departures_df from the cache\n",
    "departures_df.unpersist()\n",
    "\n",
    "# Check the cache status again\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File import performance\n",
    "\n",
    "You've been given a large set of data to import into a Spark DataFrame. You'd like to test the difference in import speed by splitting up the file.\n",
    "\n",
    "You have two types of files available: departures_full.txt.gz and departures_xxx.txt.gz where xxx is 000 - 013. The same number of rows is split between each file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Import the full and split files into DataFrames\n",
    "full_df = spark.read.csv('departures_full.txt.gz')\n",
    "split_df = spark.read.csv('departures_*.txt.gz')\n",
    "\n",
    "# Print the count and run time for each DataFrame\n",
    "start_time_a = time.time()\n",
    "print(\"Total rows in full DataFrame:\\t%d\" % full_df.count())\n",
    "print(\"Time to run: %f\" % (time.time() - start_time_a))\n",
    "\n",
    "start_time_b = time.time()\n",
    "print(\"Total rows in split DataFrame:\\t%d\" % split_df.count())\n",
    "print(\"Time to run: %f\" % (time.time() - start_time_b))\n",
    "\n",
    "\n",
    "Total rows in full DataFrame:\t139359\n",
    "Time to run: 0.212767\n",
    "Total rows in split DataFrame:\t278718\n",
    "Time to run: 0.349123\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Spark configurations\n",
    "\n",
    "You've recently configured a cluster via a cloud provider. Your only access is via the command shell or your python code. You'd like to verify some Spark settings to validate the configuration of the cluster.\n",
    "\n",
    "The spark object is available for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: clean-data\n",
      "Driver TCP port: 33563\n",
      "Number of partitions: 200\n"
     ]
    }
   ],
   "source": [
    "# Name of the Spark application instance\n",
    "app_name = spark.conf.get(\"spark.app.name\")\n",
    "\n",
    "# Driver TCP port\n",
    "driver_tcp_port = spark.conf.get(\"spark.driver.port\")\n",
    "\n",
    "# Number of join partitions\n",
    "num_partitions = spark.conf.get('spark.sql.shuffle.partitions')\n",
    "\n",
    "# Show the results\n",
    "print(\"Name: %s\" % app_name)\n",
    "print(\"Driver TCP port: %s\" % driver_tcp_port)\n",
    "print(\"Number of partitions: %s\" % num_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Spark configurations\n",
    "\n",
    "Now that you've reviewed some of the Spark configurations on your cluster, you want to modify some of the settings to tune Spark to your needs. You'll import some data to review that your changes have affected the cluster.\n",
    "\n",
    "The spark configuration is initially set to the default value of 200 partitions.\n",
    "\n",
    "The spark object is available for use. A file named departures.txt.gz is available for import. An initial DataFrame containing the distinct rows from departures.txt.gz is available as departures_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./data/AA_DFW_2014_Departures_Short.csv.gz\"\n",
    "departures_df = spark.read.format('csv').options(Header=True).load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition count before change: 1\n",
      "Partition count after change: 500\n"
     ]
    }
   ],
   "source": [
    "# Store the number of partitions in variable\n",
    "before = departures_df.rdd.getNumPartitions()\n",
    "\n",
    "# Configure Spark to use 500 partitions\n",
    "spark.conf.set('spark.sql.shuffle.partitions', 500)\n",
    "\n",
    "# Recreate the DataFrame using the departures data file\n",
    "departures_df = spark.read.csv(file_path).distinct()\n",
    "\n",
    "# Print the number of partitions for each instance\n",
    "print(\"Partition count before change: %d\" % before)\n",
    "print(\"Partition count after change: %d\" % departures_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+---+---+\n",
      "|       _c0| _c1|_c2|_c3|\n",
      "+----------+----+---+---+\n",
      "|01/02/2014|1049|RDU|138|\n",
      "|01/02/2014|1685|STL|113|\n",
      "|01/02/2014|2321|ORD|191|\n",
      "|01/03/2014|1129|IND|114|\n",
      "|01/03/2014|1407|STL| 98|\n",
      "+----------+----+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departures_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining the Spark execution plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) HashAggregate(keys=[_c2#2112], functions=[])\n",
      "+- Exchange hashpartitioning(_c2#2112, 500)\n",
      "   +- *(2) HashAggregate(keys=[_c2#2112], functions=[])\n",
      "      +- *(2) HashAggregate(keys=[_c0#2110, _c1#2111, _c2#2112, _c3#2113], functions=[])\n",
      "         +- Exchange hashpartitioning(_c0#2110, _c1#2111, _c2#2112, _c3#2113, 500)\n",
      "            +- *(1) HashAggregate(keys=[_c0#2110, _c1#2111, _c2#2112, _c3#2113], functions=[])\n",
      "               +- *(1) FileScan csv [_c0#2110,_c1#2111,_c2#2112,_c3#2113] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/gong/projects/py4kids/lesson-17-pyspark/datacamp/02_data-cleaning/da..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_c0:string,_c1:string,_c2:string,_c3:string>\n"
     ]
    }
   ],
   "source": [
    "departures_df = departures_df.select(departures_df['_c2']).distinct()\n",
    "departures_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal joins\n",
    "\n",
    "You've been given two DataFrames to combine into a single useful DataFrame. Your first task is to combine the DataFrames normally and view the execution plan.\n",
    "\n",
    "The DataFrames flights_df and airports_df are available to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./data/airports.csv.gz\"\n",
    "airports_df = spark.read.format('csv').options(Header=True).load(file_path)\n",
    "\n",
    "file_path = \"./data/flights_small.csv.gz\"\n",
    "flights_df = spark.read.format('csv').options(Header=True).load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------------+-----------------+----+---+---+\n",
      "|faa|                name|             lat|              lon| alt| tz|dst|\n",
      "+---+--------------------+----------------+-----------------+----+---+---+\n",
      "|04G|   Lansdowne Airport|      41.1304722|      -80.6195833|1044| -5|  A|\n",
      "|06A|Moton Field Munic...|      32.4605722|      -85.6800278| 264| -5|  A|\n",
      "|06C| Schaumburg Regional|      41.9893408|      -88.1012428| 801| -6|  A|\n",
      "|06N|     Randall Airport|       41.431912|      -74.3915611| 523| -5|  A|\n",
      "|09J|Jekyll Island Air...|      31.0744722|      -81.4277778|  11| -4|  A|\n",
      "|0A9|Elizabethton Muni...|      36.3712222|      -82.1734167|1593| -4|  A|\n",
      "|0G6|Williams County A...|      41.4673056|      -84.5067778| 730| -5|  A|\n",
      "|0G7|Finger Lakes Regi...|      42.8835647|      -76.7812318| 492| -5|  A|\n",
      "|0P2|Shoestring Aviati...|      39.7948244|      -76.6471914|1000| -5|  U|\n",
      "|0S9|Jefferson County ...|      48.0538086|     -122.8106436| 108| -8|  A|\n",
      "|0W3|Harford County Ai...|      39.5668378|      -76.2024028| 409| -5|  A|\n",
      "|10C|  Galt Field Airport|      42.4028889|      -88.3751111| 875| -6|  U|\n",
      "|17G|Port Bucyrus-Craw...|      40.7815556|      -82.9748056|1003| -5|  A|\n",
      "|19A|Jackson County Ai...|      34.1758638|      -83.5615972| 951| -4|  U|\n",
      "|1A3|Martin Campbell F...|      35.0158056|      -84.3468333|1789| -4|  A|\n",
      "|1B9| Mansfield Municipal|      42.0001331|      -71.1967714| 122| -5|  A|\n",
      "|1C9|Frazier Lake Airpark|54.0133333333333|-124.768333333333| 152| -8|  A|\n",
      "|1CS|Clow Internationa...|      41.6959744|      -88.1292306| 670| -6|  U|\n",
      "|1G3|  Kent State Airport|      41.1513889|      -81.4151111|1134| -4|  A|\n",
      "|1OH|     Fortman Airport|      40.5553253|      -84.3866186| 885| -5|  U|\n",
      "+---+--------------------+----------------+-----------------+----+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|faa|\n",
      "+---+\n",
      "|JFK|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports_df.where(airports_df['name'].contains(\"Kennedy\")).select(\"faa\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2014|   12|  8|     658|       -7|     935|       -5|     VX| N846VA|  1780|   SEA| LAX|     132|     954|   6|    58|\n",
      "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|\n",
      "|2014|    3|  9|    1443|       -2|    1652|        2|     VX| N847VA|   755|   SEA| SFO|     111|     679|  14|    43|\n",
      "|2014|    4|  9|    1705|       45|    1839|       34|     WN| N360SW|   344|   PDX| SJC|      83|     569|  17|     5|\n",
      "|2014|    3|  9|     754|       -1|    1015|        1|     AS| N612AS|   522|   SEA| BUR|     127|     937|   7|    54|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) BroadcastHashJoin [dest#2185], [faa#2150], Inner, BuildRight\n",
      ":- *(2) Project [year#2174, month#2175, day#2176, dep_time#2177, dep_delay#2178, arr_time#2179, arr_delay#2180, carrier#2181, tailnum#2182, flight#2183, origin#2184, dest#2185, air_time#2186, distance#2187, hour#2188, minute#2189]\n",
      ":  +- *(2) Filter isnotnull(dest#2185)\n",
      ":     +- *(2) FileScan csv [year#2174,month#2175,day#2176,dep_time#2177,dep_delay#2178,arr_time#2179,arr_delay#2180,carrier#2181,tailnum#2182,flight#2183,origin#2184,dest#2185,air_time#2186,distance#2187,hour#2188,minute#2189] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/gong/projects/py4kids/lesson-17-pyspark/datacamp/02_data-cleaning/da..., PartitionFilters: [], PushedFilters: [IsNotNull(dest)], ReadSchema: struct<year:string,month:string,day:string,dep_time:string,dep_delay:string,arr_time:string,arr_d...\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n",
      "   +- *(1) Project [faa#2150, name#2151, lat#2152, lon#2153, alt#2154, tz#2155, dst#2156]\n",
      "      +- *(1) Filter isnotnull(faa#2150)\n",
      "         +- *(1) FileScan csv [faa#2150,name#2151,lat#2152,lon#2153,alt#2154,tz#2155,dst#2156] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/gong/projects/py4kids/lesson-17-pyspark/datacamp/02_data-cleaning/da..., PartitionFilters: [], PushedFilters: [IsNotNull(faa)], ReadSchema: struct<faa:string,name:string,lat:string,lon:string,alt:string,tz:string,dst:string>\n"
     ]
    }
   ],
   "source": [
    "# Join the flights_df and aiports_df DataFrames\n",
    "normal_df = flights_df.join(airports_df, \\\n",
    "    flights_df[\"dest\"] == airports_df[\"faa\"] )\n",
    "\n",
    "# Show the query plan\n",
    "normal_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using broadcasting on Spark joins\n",
    "\n",
    "Remember that table joins in Spark are split between the cluster workers. If the data is not local, various shuffle operations are required and can have a negative impact on performance. Instead, we're going to use Spark's broadcast operations to give each node a copy of the specified data.\n",
    "\n",
    "A couple tips:\n",
    "\n",
    "*    Broadcast the smaller DataFrame. The larger the DataFrame, the more time required to transfer to the worker nodes.\n",
    "*    On small DataFrames, it may be better skip broadcasting and let Spark figure out any optimization on its own.\n",
    "*    If you look at the query execution plan, a broadcastHashJoin indicates you've successfully configured broadcasting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) BroadcastHashJoin [dest#2185], [faa#2150], Inner, BuildRight\n",
      ":- *(2) Project [year#2174, month#2175, day#2176, dep_time#2177, dep_delay#2178, arr_time#2179, arr_delay#2180, carrier#2181, tailnum#2182, flight#2183, origin#2184, dest#2185, air_time#2186, distance#2187, hour#2188, minute#2189]\n",
      ":  +- *(2) Filter isnotnull(dest#2185)\n",
      ":     +- *(2) FileScan csv [year#2174,month#2175,day#2176,dep_time#2177,dep_delay#2178,arr_time#2179,arr_delay#2180,carrier#2181,tailnum#2182,flight#2183,origin#2184,dest#2185,air_time#2186,distance#2187,hour#2188,minute#2189] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/gong/projects/py4kids/lesson-17-pyspark/datacamp/02_data-cleaning/da..., PartitionFilters: [], PushedFilters: [IsNotNull(dest)], ReadSchema: struct<year:string,month:string,day:string,dep_time:string,dep_delay:string,arr_time:string,arr_d...\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n",
      "   +- *(1) Project [faa#2150, name#2151, lat#2152, lon#2153, alt#2154, tz#2155, dst#2156]\n",
      "      +- *(1) Filter isnotnull(faa#2150)\n",
      "         +- *(1) FileScan csv [faa#2150,name#2151,lat#2152,lon#2153,alt#2154,tz#2155,dst#2156] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/gong/projects/py4kids/lesson-17-pyspark/datacamp/02_data-cleaning/da..., PartitionFilters: [], PushedFilters: [IsNotNull(faa)], ReadSchema: struct<faa:string,name:string,lat:string,lon:string,alt:string,tz:string,dst:string>\n"
     ]
    }
   ],
   "source": [
    "# Import the broadcast method from pyspark.sql.functions\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Join the flights_df and aiports_df DataFrames using broadcasting\n",
    "broadcast_df = flights_df.join(broadcast(airports_df), \\\n",
    "    flights_df[\"dest\"] == airports_df[\"faa\"] )\n",
    "\n",
    "# Show the query plan and compare against the original\n",
    "broadcast_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing broadcast vs normal joins\n",
    "\n",
    "You've created two types of joins, normal and broadcasted. Now your manager would like to know what the performance improvement is by using Spark optimizations. If the results are promising, you'll be given more opportunity to tweak the Spark setup as needed.\n",
    "\n",
    "Your DataFrames normal_df and broadcast_df are available for your use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "start_time = time.time()\n",
    "# Count the number of rows in the normal DataFrame\n",
    "normal_count = normal_df.count()\n",
    "normal_duration = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "# Count the number of rows in the broadcast DataFrame\n",
    "broadcast_count = broadcast_df.count()\n",
    "broadcast_duration = time.time() - start_time\n",
    "\n",
    "# Print the counts and the duration of the tests\n",
    "print(\"Normal count:\\t\\t%d\\tduration: %f\" % (normal_count, normal_duration))\n",
    "print(\"Broadcast count:\\t%d\\tduration: %f\" % (broadcast_count, broadcast_duration))\n",
    "\n",
    "\n",
    "Normal count:\t\t119910\tduration: 1.571761\n",
    "Broadcast count:\t119910\tduration: 0.340586\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick pipeline\n",
    "\n",
    "Before you parse some more complex data, your manager would like to see a simple pipeline example including the basic steps. For this example, you'll want to ingest a data file, filter a few rows, add an ID column to it, then write it out as JSON data.\n",
    "\n",
    "The spark context is defined, along with the pyspark.sql.functions library being aliased as F as is customary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./data/AA_DFW_2015_Departures_Short.csv.gz\"\n",
    "departures_df = spark.read.format('csv').options(Header=True).load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|       01/01/2015|         0005|                HNL|                          526|\n",
      "|       01/01/2015|         0007|                OGG|                          517|\n",
      "|       01/01/2015|         0023|                SFO|                          233|\n",
      "|       01/01/2015|         0027|                LAS|                          165|\n",
      "|       01/01/2015|         0035|                HDN|                          178|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departures_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any duration of 0\n",
    "departures_df = departures_df.filter(departures_df['Actual elapsed time (Minutes)'] > 0)\n",
    "\n",
    "# Add an ID column\n",
    "departures_df = departures_df.withColumn('id', F.monotonically_increasing_id())\n",
    "\n",
    "# Write the file out to JSON format\n",
    "departures_df.write.json(\"output.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing commented lines\n",
    "\n",
    "Your boss would like you to perform some complex parsing on a new dataset. The data represents annotation data for the ImageNet dataset, but focusing specifically on dog breeds and identifying them in images. Before any actual analysis can occur, you'll need to clear out several components of invalid / incorrect data. The general schema of the document is unknown so you'd like to import the rows into a single column, allowing for quick analysis.\n",
    "\n",
    "To start, you need to remove all commented rows in the dataset.\n",
    "\n",
    "The spark context, and the base CSV file (annotations.csv.gz) are available for you to work with. The col function is also available for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the file to a DataFrame and perform a row count\n",
    "annotations_df = spark.read.csv('annotations.csv.gz', sep='|')\n",
    "full_count = annotations_df.count()\n",
    "\n",
    "# Count the number of rows beginning with '#'\n",
    "comment_count = annotations_df.filter(col('_c0').startswith('#')).count()\n",
    "\n",
    "# Import the file to a new DataFrame, without commented rows\n",
    "no_comments_df = annotations_df.filter(~col('_c0').startswith('#')).count()\n",
    "\n",
    "# Count the new DataFrame and verify the difference is as expected\n",
    "no_comments_count = no_comments_df.count()\n",
    "print(\"Full count: %d\\nComment count: %d\\nRemaining count: %d\" % (full_count, comment_count, (full_count-comment_count)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing invalid rows\n",
    "\n",
    "Now that you've successfully removed the commented rows, you have received some information about the general format of the data. There should be at minimum 5 tab separated columns in the DataFrame. Remember that your original DataFrame only has a single column, so you'll need to split the data on the tab (\\t) characters.\n",
    "\n",
    "The DataFrame annotations_df is already available, with the commented rows removed. The spark.sql.functions library is available under the alias F. The initial number of rows available in the DataFrame is stored in the variable initial_count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Split _c0 on the tab character and store the list in a variable\n",
    "tmp_fields = F.split(annotations_df['_c0'], '\\t')\n",
    "\n",
    "# Create the colcount column on the DataFrame\n",
    "annotations_df = annotations_df.withColumn('colcount', F.size(tmp_fields))\n",
    "\n",
    "# Remove any rows containing fewer than 5 fields\n",
    "annotations_df_filtered = annotations_df.filter(~ (annotations_df['colcount'] < 5))\n",
    "\n",
    "# Count the number of rows\n",
    "final_count = annotations_df_filtered.count()\n",
    "print(\"Initial count: %d\\nFinal count: %d\" % (initial_count, final_count))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into columns\n",
    "\n",
    "You've cleaned up your data considerably by removing the invalid rows from the DataFrame. Now you want to perform some further transformations by generating specific meaningful columns based on the DataFrame content.\n",
    "\n",
    "You may be wondering why we're not using a schema instead to define the content layout. Spark's CSV parser can't handle advanced types (Arrays or Maps) so it wouldn't process correctly. In our example, we bypass using the types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Split the content of _c0 on the tab character (aka, '\\t')\n",
    "split_cols = F.split(annotations_df['_c0'], '\\t')\n",
    "\n",
    "# Add the columns folder, filename, width, and height\n",
    "split_df = annotations_df.withColumn('folder', split_cols.getItem(0))\n",
    "split_df = split_df.withColumn('filename', split_cols.getItem(1))\n",
    "split_df = split_df.withColumn('width', split_cols.getItem(2))\n",
    "split_df = split_df.withColumn('height', split_cols.getItem(3))\n",
    "\n",
    "# Add split_cols as a column\n",
    "split_df = split_df.withColumn('split_cols', split_cols)\n",
    "\n",
    "split_df.head(5)\n",
    "\n",
    "split_df.printSchema()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further parsing\n",
    "\n",
    "You've molded this dataset into a significantly different format than it was before, but there are still a few things left to do. You need to prep the column data for use in later analysis and remove a few intermediary columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def retriever(cols, colcount):\n",
    "    # Return a list of dog data\n",
    "    return cols[4:colcount]\n",
    "\n",
    "# Define the method as a UDF\n",
    "udfRetriever = F.udf(retriever, ArrayType(StringType()))\n",
    "\n",
    "# Create a new column using your UDF\n",
    "split_df = split_df.withColumn('dog_list', udfRetriever(col('split_cols'), col('colcount')))\n",
    "\n",
    "# Remove the original column, split_cols, and the colcount\n",
    "split_df = split_df.drop('_c0').drop('split_cols').drop('colcount')\n",
    "\n",
    "\n",
    "\n",
    "In [1]: split_df.show(2)\n",
    "+--------+---------------+-----+------+--------------------+\n",
    "|  folder|       filename|width|height|            dog_list|\n",
    "+--------+---------------+-----+------+--------------------+\n",
    "|02110627|n02110627_12938|  200|   300|[affenpinscher,0,...|\n",
    "|02093754| n02093754_1148|  500|   378|[Border_terrier,7...|\n",
    "+--------+---------------+-----+------+--------------------+\n",
    "only showing top 2 rows\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate rows via join\n",
    "\n",
    "Another example of filtering data is using joins to remove invalid entries. You'll need to verify the folder names are as expected based on a given DataFrame named valid_folders_df. The DataFrame split_df is as you last left it with a group of split columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Rename the column in valid_folders_df\n",
    "valid_folders_df = valid_folders_df.withColumnRenamed(\"_c0\",\"folder\")\n",
    "\n",
    "# Count the number of rows in split_df\n",
    "split_count = split_df.count()\n",
    "\n",
    "# Join the DataFrames\n",
    "joined_df = split_df.join(F.broadcast(valid_folders_df), split_df.folder == valid_folders_df.folder)\n",
    "\n",
    "# Compare the number of rows remaining\n",
    "joined_count = joined_df.count()\n",
    "print(\"Before: %d\\nAfter: %d\" % (split_count, joined_count))\n",
    "\n",
    "\n",
    "\n",
    "Before: 20580\n",
    "After: 19956\n",
    "\n",
    "In [5]: joined_df.show(2)\n",
    "+--------+---------------+-----+------+--------------------+--------+\n",
    "|  folder|       filename|width|height|            dog_list|  folder|\n",
    "+--------+---------------+-----+------+--------------------+--------+\n",
    "|02110627|n02110627_12938|  200|   300|[affenpinscher,0,...|02110627|\n",
    "|02093754| n02093754_1148|  500|   378|[Border_terrier,7...|02093754|\n",
    "+--------+---------------+-----+------+--------------------+--------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dog parsing\n",
    "\n",
    "You've done a considerable amount of cleanup on the initial dataset, but now need to analyze the data a bit deeper. There are several questions that have now come up about the type of dogs seen in an image and some details regarding the images. You realize that to answer these questions, you need to process the data into a specific type. Before you can use it, you'll need to create a schema / type to represent the dog details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Select the dog details and show 10 untruncated rows\n",
    "print(joined_df.select('dog_list').show(truncate=False))\n",
    "\n",
    "# Define a schema type for the details in the dog list\n",
    "DogType = StructType([\n",
    "\tStructField(\"breed\", StringType(), False),\n",
    "    StructField(\"start_x\", IntegerType(), False),\n",
    "    StructField(\"start_y\", IntegerType(), False),\n",
    "    StructField(\"end_x\", IntegerType(), False),\n",
    "    StructField(\"end_y\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "joined_df.select('dog_list', F.size(F.col('dog_list'))).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per image count\n",
    "\n",
    "Your next task in building a data pipeline for this dataset is to create a few analysis oriented columns. You've been asked to calculate the number of dogs found in each image based on your dog_list column created earlier. You have also created the DogType which will allow better parsing of the data within some of the data columns.\n",
    "\n",
    "The joined_df is available as you last defined it, and the DogType structtype is defined. pyspark.sql.functions is available under the F alias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Create a function to return the number and type of dogs as a tuple\n",
    "def dogParse(doglist):\n",
    "  dogs = []\n",
    "  for dog in doglist:\n",
    "    (breed, start_x, start_y, end_x, end_y) = dog.split(',')\n",
    "    dogs.append((breed, int(start_x), int(start_y), int(end_x), int(end_y)))\n",
    "  return dogs\n",
    "\n",
    "# Create a UDF\n",
    "udfDogParse = F.udf(dogParse, ArrayType(DogType))\n",
    "\n",
    "# Use the UDF to list of dogs and drop the old column\n",
    "joined_df = joined_df.withColumn('dogs', udfDogParse('dog_list')).drop('dog_list')\n",
    "\n",
    "# Show the number of dogs in the first 10 rows\n",
    "joined_df.select(F.size('dogs')).show(10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
