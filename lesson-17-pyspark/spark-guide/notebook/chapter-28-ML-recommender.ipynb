{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"chapter-28-ML-recommender\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "import os\n",
    "SPARK_BOOK_DATA_PATH = os.environ['SPARK_BOOK_DATA_PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: alpha for implicit preference (default: 1.0)\n",
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
      "coldStartStrategy: strategy for dealing with unknown or new users/items at prediction time. This may be useful in cross-validation or production scenarios, for handling user/item ids the model has not seen in the training data. Supported values: 'nan', 'drop'. (default: nan)\n",
      "finalStorageLevel: StorageLevel for ALS model factors. (default: MEMORY_AND_DISK)\n",
      "implicitPrefs: whether to use implicit preference (default: False)\n",
      "intermediateStorageLevel: StorageLevel for intermediate datasets. Cannot be 'NONE'. (default: MEMORY_AND_DISK)\n",
      "itemCol: column name for item ids. Ids must be within the integer value range. (default: item, current: movieId)\n",
      "maxIter: max number of iterations (>= 0). (default: 10, current: 5)\n",
      "nonnegative: whether to use nonnegative constraint for least squares (default: False)\n",
      "numItemBlocks: number of item blocks (default: 10)\n",
      "numUserBlocks: number of user blocks (default: 10)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "rank: rank of the factorization (default: 10)\n",
      "ratingCol: column name for ratings (default: rating, current: rating)\n",
      "regParam: regularization parameter (>= 0). (default: 0.1, current: 0.01)\n",
      "seed: random seed. (default: -1517157561977538513)\n",
      "userCol: column name for user ids. Ids must be within the integer value range. (default: user, current: userId)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "ratings = spark.read\\\n",
    "    .text(SPARK_BOOK_DATA_PATH + \"/data/sample_movielens_ratings.txt\")\\\n",
    "    .rdd.toDF()\\\n",
    "    .selectExpr(\"split(value , '::') as col\")\\\n",
    "    .selectExpr(\n",
    "        \"cast(col[0] as int) as userId\",\n",
    "        \"cast(col[1] as int) as movieId\",\n",
    "        \"cast(col[2] as float) as rating\",\n",
    "        \"cast(col[3] as long) as timestamp\"\n",
    "    )\n",
    "\n",
    "training, test = ratings.randomSplit([0.8, 0.2])\n",
    "\n",
    "als = ALS()\\\n",
    "  .setMaxIter(5)\\\n",
    "  .setRegParam(0.01)\\\n",
    "  .setUserCol(\"userId\")\\\n",
    "  .setItemCol(\"movieId\")\\\n",
    "  .setRatingCol(\"rating\")\n",
    "\n",
    "print (als.explainParams())\n",
    "alsModel = als.fit(training)               # fit to train Model on training data\n",
    "predictions = alsModel.transform(test)     # transform to predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+\n",
      "|userId|            col|\n",
      "+------+---------------+\n",
      "|    28| [12, 4.854412]|\n",
      "|    28|[81, 4.5314903]|\n",
      "|    28|   [2, 3.90854]|\n",
      "|    28| [82, 3.842551]|\n",
      "|    28|[23, 3.5598414]|\n",
      "|    28|[76, 3.4648323]|\n",
      "|    28| [62, 3.214014]|\n",
      "|    28| [26, 2.959738]|\n",
      "|    28|[57, 2.8760154]|\n",
      "|    28|[70, 2.7861943]|\n",
      "|    26|[83, 5.9210377]|\n",
      "|    26|[33, 5.4730806]|\n",
      "|    26| [19, 5.261099]|\n",
      "|    26| [37, 5.150425]|\n",
      "|    26| [24, 5.059302]|\n",
      "|    26|[12, 5.0024195]|\n",
      "|    26| [88, 4.989318]|\n",
      "|    26|[22, 4.9702883]|\n",
      "|    26| [94, 4.957751]|\n",
      "|    26|[23, 4.8820114]|\n",
      "+------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "alsModel.recommendForAllUsers(10)\\\n",
    "  .selectExpr(\"userId\", \"explode(recommendations)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+\n",
      "|movieId|            col|\n",
      "+-------+---------------+\n",
      "|     31| [21, 4.190279]|\n",
      "|     31|[12, 3.6101668]|\n",
      "|     31| [6, 3.0709796]|\n",
      "|     31| [8, 3.0144005]|\n",
      "|     31|[14, 3.0032687]|\n",
      "|     31| [7, 2.8663979]|\n",
      "|     31|[13, 2.4790258]|\n",
      "|     31|[10, 2.4686954]|\n",
      "|     31| [25, 2.211947]|\n",
      "|     31| [4, 2.1117184]|\n",
      "|     85|  [8, 4.849661]|\n",
      "|     85| [16, 4.708046]|\n",
      "|     85| [7, 3.7650425]|\n",
      "|     85| [6, 3.5269358]|\n",
      "|     85|    [1, 3.0007]|\n",
      "|     85|[14, 2.8658288]|\n",
      "|     85| [21, 2.746056]|\n",
      "|     85|[19, 2.5981874]|\n",
      "|     85|[20, 2.3025043]|\n",
      "|     85| [10, 2.057417]|\n",
      "+-------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alsModel.recommendForAllItems(10)\\\n",
    "  .selectExpr(\"movieId\", \"explode(recommendations)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 1.544025\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator()\\\n",
    "  .setMetricName(\"rmse\")\\\n",
    "  .setLabelCol(\"rating\")\\\n",
    "  .setPredictionCol(\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = %f\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "regComparison = predictions.select(\"rating\", \"prediction\")\\\n",
    "  .rdd.map(lambda x: (x(0), x(1)))\n",
    "metrics = RegressionMetrics(regComparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.mllib.evaluation import RankingMetrics, RegressionMetrics\n",
    "from pyspark.sql.functions import col, expr\n",
    "perUserActual = predictions\\\n",
    "  .where(\"rating > 2.5\")\\\n",
    "  .groupBy(\"userId\")\\\n",
    "  .agg(expr(\"collect_set(movieId) as movies\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "perUserPredictions = predictions\\\n",
    "  .orderBy(col(\"userId\"), expr(\"prediction DESC\"))\\\n",
    "  .groupBy(\"userId\")\\\n",
    "  .agg(expr(\"collect_list(movieId) as movies\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "perUserActualvPred = perUserActual.join(perUserPredictions, [\"userId\"]).rdd\\\n",
    "  .map(lambda row: (row[1], row[2][:15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(perUserActualvPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([19, 49, 89, 92], [65, 19, 44, 3, 59, 49, 13, 89, 92]),\n",
       " ([18, 36, 73], [61, 18, 73, 36, 91]),\n",
       " ([75, 55, 44], [31, 44, 42, 95, 52, 55, 75]),\n",
       " ([66, 64, 50], [83, 66, 44, 84, 63, 4, 15, 88, 60, 67, 50, 64, 82, 87]),\n",
       " ([30, 69, 22], [14, 44, 45, 90, 22, 30, 63, 69, 96]),\n",
       " ([72], [78, 87, 73, 98, 26, 85, 31, 72, 71]),\n",
       " ([43], [75, 44, 38, 33, 18, 16, 68, 22, 94, 56, 20, 43, 53]),\n",
       " ([90], [90, 49, 38, 15, 65, 60, 2, 21]),\n",
       " ([36, 8, 80], [8, 91, 89, 81, 46, 83, 36, 72, 80, 62]),\n",
       " ([51, 90], [1, 73, 90, 78, 25, 24, 51]),\n",
       " ([54], [26, 2, 4, 11, 45, 61, 33, 54, 6, 55]),\n",
       " ([87, 14], [98, 14, 87, 77]),\n",
       " ([46, 56, 55], [70, 46, 56, 55, 93, 29, 66, 72, 19, 34]),\n",
       " ([96], [54, 96, 79, 33, 83, 4, 71, 15, 56, 9]),\n",
       " ([30, 13, 68, 55, 73, 23], [28, 59, 23, 68, 61, 13, 0, 73, 30, 55]),\n",
       " ([42], [66, 90, 42, 43, 98, 37, 8, 58, 41]),\n",
       " ([30, 96, 68, 69], [84, 78, 31, 96, 10, 30, 69, 68]),\n",
       " ([49], [70, 42, 49]),\n",
       " ([96, 29], [55, 20, 38, 66, 46, 40, 45, 51, 96, 29]),\n",
       " ([66, 27, 13, 50, 23],\n",
       "  [62, 70, 13, 45, 50, 43, 77, 66, 89, 6, 11, 88, 27, 23, 59]),\n",
       " ([52, 53, 93, 3, 72, 47],\n",
       "  [48, 49, 36, 39, 93, 6, 83, 47, 53, 5, 3, 95, 94, 45, 72]),\n",
       " ([83, 4], [76, 35, 78, 4, 83]),\n",
       " ([9, 92], [98, 9, 17, 45, 95, 34, 59, 92, 29])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perUserActualvPred.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.29808913308913304, 0.49565217391304356)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks = RankingMetrics(perUserActualvPred)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "ranks.meanAveragePrecision, ranks.precisionAt(5)\n",
    "\n",
    "\n",
    "# COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
