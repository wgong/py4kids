# Ollama Upgrade Issue Fix - September 23, 2025

## Problem Diagnosis

After upgrading Ollama to version 0.12.0, the systemd service failed to start with the following error:

```
Error: open /usr/share/ollama/.ollama/id_ed25519: permission denied
```

The service was stuck in a restart loop (457+ attempts) because it was trying to access the default model location `/usr/share/ollama/.ollama/` instead of the custom location where models are actually stored.

## Root Cause

1. **Ollama upgrade reset configuration**: The upgrade to v0.12.0 removed the custom `OLLAMA_MODELS` environment variable from the systemd service
2. **Models location mismatch**: Models are stored in `/home/papagame/.ollama/models/` but service was looking in `/usr/share/ollama/.ollama/`
3. **Permission issue**: The `ollama` user doesn't have write permissions to the default system location

## Current Status Verification

- **Ollama version**: 0.12.0 (upgraded from 0.7.1)
- **Models location**: `/home/papagame/.ollama/models/` (28 models available)
- **Manual ollama serve**: Works correctly when run manually
- **Service status**: Failing due to missing environment variable

## Solution Steps

### 1. Copy Fixed Service Configuration
The first attempt failed because the `ollama` user doesn't have permission to write SSH keys to `/usr/share/ollama/.ollama/`.

Try the updated fix that runs as `papagame` user:
```bash
sudo cp -f /home/papagame/projects/wgong/py4kids/lesson-18-ai/ollama/ollama.service.fixed2 /etc/systemd/system/ollama.service
```

### 2. Kill any existing ollama processes and restart service
```bash
# Kill any existing ollama processes to avoid port conflicts
pkill -f ollama

# Reload SystemD and restart service
sudo systemctl daemon-reload
sudo systemctl restart ollama
```

### 3. Verify Service Status
```bash
sudo systemctl status ollama
```

run step-3 a few times, until one sees:
```
● ollama.service - Ollama Service
     Loaded: loaded (/etc/systemd/system/ollama.service; enabled; vendor preset: enabled)
     Active: active (running) since Tue 2025-09-23 15:18:13 EDT; 1ms ago
   Main PID: 33993 ((ollama))
      Tasks: 1 (limit: 57615)
     Memory: 0B
        CPU: 0
     CGroup: /system.slice/ollama.service
             └─33993 "[(ollama)]"
```

### 4. Check Service Logs
```bash
sudo journalctl -u ollama.service -n 20 --no-pager
```


```
Sep 23 15:30:30 papa-game systemd[1]: Stopping Ollama Service...
Sep 23 15:30:30 papa-game systemd[1]: ollama.service: Deactivated successfully.
Sep 23 15:30:30 papa-game systemd[1]: Stopped Ollama Service.
Sep 23 15:30:30 papa-game systemd[1]: Started Ollama Service.
Sep 23 15:30:30 papa-game ollama[45217]: time=2025-09-23T15:30:30.731-04:00 level=INFO source=routes.go:1466 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/papagame/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
Sep 23 15:30:30 papa-game ollama[45217]: time=2025-09-23T15:30:30.736-04:00 level=INFO source=images.go:518 msg="total blobs: 136"
Sep 23 15:30:30 papa-game ollama[45217]: time=2025-09-23T15:30:30.737-04:00 level=INFO source=images.go:525 msg="total unused blobs removed: 0"
Sep 23 15:30:30 papa-game ollama[45217]: time=2025-09-23T15:30:30.737-04:00 level=INFO source=routes.go:1519 msg="Listening on 127.0.0.1:11434 (version 0.12.0)"
Sep 23 15:30:30 papa-game ollama[45217]: time=2025-09-23T15:30:30.738-04:00 level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
Sep 23 15:30:30 papa-game ollama[45217]: time=2025-09-23T15:30:30.852-04:00 level=INFO source=types.go:131 msg="inference compute" id=GPU-7e54064f-5498-250d-d6f5-696ed48dca43 library=cuda variant=v12 compute=6.1 driver=12.4 name="NVIDIA GeForce GTX 1080 Ti" total="10.9 GiB" available="10.2 GiB"
Sep 23 15:30:30 papa-game ollama[45217]: time=2025-09-23T15:30:30.852-04:00 level=INFO source=routes.go:1560 msg="entering low vram mode" "total vram"="10.9 GiB" threshold="20.0 GiB"


```

### 5. Test Model Access
```bash
ollama list
ollama run gemma3 "What is 2+2?"
```

## Key Fix Details

The fixed service file (`ollama.service.fixed`) adds the missing environment variable:

```ini
Environment="OLLAMA_MODELS=/home/papagame/.ollama/models"
```

This tells the ollama service where to find the models and where to store configuration files like the SSH key.

## Prevention for Future Upgrades

**Important**: After any ollama upgrade, check if the systemd service configuration was reset. The key environment variables that might be lost are:

- `OLLAMA_MODELS=/home/papagame/.ollama/models`
- Any custom performance settings like `OLLAMA_NUM_PARALLEL` or `OLLAMA_MAX_LOADED_MODELS`

## Verification Checklist

- [ ] Service starts without errors
- [ ] No permission denied errors in logs
- [ ] `ollama list` shows all 28 models
- [ ] Model inference works (test with gemma3 or phi4)
- [ ] GPU detection working (GTX 1080 Ti with 10.9GB VRAM)

## Notes

- This is the same pattern as the May 24, 2025 upgrade issue
- Models were already in the correct location from the previous fix
- The upgrade only affected the systemd service configuration, not the models themselves
- Manual `ollama serve` worked because it uses the current user's environment